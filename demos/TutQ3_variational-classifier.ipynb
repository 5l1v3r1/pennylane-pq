{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - Variational classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tutorial 3 we show how to use openqml to implement variational quantum classifiers - quantum circuits that can be trained from labelled data how to classify new data samples. The architecture is inspired by Farhi & Neven (2018 arXiv:1802.06002) as well as Schuld, Bocharov, Wiebe and Svore (2018 arXiv:1804.00633). \n",
    "\n",
    "We will first show that the variational quantum classifier can reproduce the parity function\n",
    "\n",
    "$$ f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y = \\begin{cases} 1 \\text{  if uneven number of ones in } x \\\\ 0 \\text{ else} \\end{cases}.$$\n",
    "\n",
    "This optimization example is supposed to demonstrate how to encode binary inputs into the initial state of the variational circuit, which is simply a computational basis state.\n",
    "\n",
    "We then show how to encode real vectors as amplitude vectors (*amplitude encoding*) and train the model to recognise the first two classes of flowers in the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fitting the parity function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openqml as qm\n",
    "from openqml import numpy as onp\n",
    "import numpy as np\n",
    "from openqml.optimize import AdagradOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a quantum device with four \"wires\" or qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qm.device('default.qubit', wires=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational classifiers usually define a \"layer\" or \"block\", which is an elementary circuit architecture that gets repeated to build the variational circuit.\n",
    "\n",
    "<IMage>\n",
    "\n",
    "Our circuit layer consists of an arbitrary rotation on every qubit, as well as CNOTs that entangle each qubit with its neighbour.\n",
    "\n",
    "<IMage>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "\n",
    "    qm.Rot(W[0, 0], W[0, 1], W[0, 2], [0])\n",
    "    qm.Rot(W[1, 0], W[1, 1], W[1, 2], [1])\n",
    "    qm.Rot(W[2, 0], W[2, 1], W[2, 2], [2])\n",
    "    qm.Rot(W[3, 0], W[3, 1], W[3, 2], [3])\n",
    "\n",
    "    qm.CNOT([0, 1])\n",
    "    qm.CNOT([1, 2])\n",
    "    qm.CNOT([2, 3])\n",
    "    qm.CNOT([3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to encode data inputs $x$ into the circuit, so that the measured output depends on the inputs. In this first example, the inputs are bitstrings, which we encode into the state of the qubits. The quantum state $|\\psi \\rangle $ after state preparation is a computational basis state that has 1's where $x$ has 1s, for example\n",
    "\n",
    "$$ x = 0101 \\rightarrow |\\psi \\rangle = |0101 \\rangle . $$\n",
    "\n",
    "We use the `BasisState` function, which expects `x` to be a list of zeros and ones, i.e. `[0,1,0,1]`.\n",
    "\n",
    "*Note: OpenQML wraps the arguemtns of a quantum node, so they cannot be evaluated directly by the user. For example, this will NOT work:*\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 1:\n",
    "            qm.PauliX([i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x):\n",
    "\n",
    "    qm.BasisState(x, wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can elegantly define the quantum node as a state preparation routine, followed by a repitition of the layer structure. Borrowing from machine learning, we call the parameters `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qm.qnode(dev)\n",
    "def circuit(weights, x=None):\n",
    "\n",
    "    statepreparation(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qm.expval.PauliZ(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from previous tutorials, the quantum node takes the data as a keyword argument `x` (with the default value `None`). Keyword arguments of a quantum node are considered as fixed when calculating a gradient, so they are never trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to add a \"classical\" bias parameter, the variational quantum classifer also needs some post-processing. We define the final model by a classical node that uses the first variable, and feeds the remainder into the quantum node. Before this, we reshape the list of remaining variables for easy use in the quantum node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(vars, x=None, shape=None):\n",
    "\n",
    "    weights = onp.reshape(vars[1:], shape)\n",
    "    outp = circuit(weights, x=x)\n",
    "\n",
    "    return outp + vars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective or cost in supervised learning is the sum of a loss and a regularizer. We use the standard square-loss that measures the distance between target labels and model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l-p)**2\n",
    "    loss = loss/len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor how many inputs the current classifier predicted correctly, we also define the accuracy given target labels and model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l-p) < 1e-5:\n",
    "            loss += 1\n",
    "    loss = loss/len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning tasks, the cost depends on the data - here the features and labels considered in the iteration of the optimization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, features, labels, shape=None):\n",
    "\n",
    "    predictions = [variational_classifier(weights, x=f, shape=shape) for f in features]\n",
    "\n",
    "    return square_loss(labels, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load and preprocess some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [0. 0. 0. 0.], Y = -1\n",
      "X = [0. 0. 0. 1.], Y =  1\n",
      "X = [0. 0. 1. 0.], Y =  1\n",
      "X = [0. 0. 1. 1.], Y = -1\n",
      "X = [0. 1. 0. 0.], Y =  1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"parity.txt\")\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "Y = Y*2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for i in range(5):\n",
    "    print('X = {}, Y = {: d}'.format(X[i], int(Y[i])))\n",
    "print('...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables randomly. The first variable in the list is used as a bias, while the rest is fed into the gates of the variational circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00905118, -0.02874512, -0.00481697, -0.00214992,  0.00365692,\n",
       "       -0.0042674 ,  0.01175576, -0.02070308,  0.0030833 ,  0.00140951,\n",
       "       -0.00352826, -0.01086024, -0.00698275, -0.00913977, -0.02521236,\n",
       "       -0.00460899, -0.01045835, -0.00162891, -0.0111162 , -0.00258407,\n",
       "       -0.00739962, -0.00221939, -0.006418  ,  0.0116421 ,  0.01146174])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "vars_init = 0.01*np.random.randn(num_qubits*3*num_layers+1)\n",
    "shp = (num_layers, num_qubits, 3)\n",
    "\n",
    "vars_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer and choose a batch size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = AdagradOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and train the optimizer. We track the accuracy - the share of correctly classified data samples. For this we compute the outputs of the variational classifier and turn them into predictions in $\\{-1,1\\}$ by taking the sign of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.5312400 | Accuracy: 0.3750000 \n",
      "Iter:     2 | Cost: 1.1661283 | Accuracy: 0.5000000 \n",
      "Iter:     3 | Cost: 0.7626209 | Accuracy: 0.8125000 \n",
      "Iter:     4 | Cost: 0.4389153 | Accuracy: 1.0000000 \n",
      "Iter:     5 | Cost: 0.0476631 | Accuracy: 1.0000000 \n"
     ]
    }
   ],
   "source": [
    "vars = vars_init\n",
    "for it in range(5):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size, ))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    vars = o.step(lambda v: cost(v, X, Y, shape=shp), vars)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(vars, x=x, shape=shp)) for x in X]\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(\"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \"\n",
    "          \"\".format(it+1, cost(vars, X, Y), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Iris classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encoding real-valued vectors into the amplitudes of a quantum state, we simply overwrite the statepreparation function. Since we use four qubits, the quantum state has 16 amplitudes, and we encode $x$ into the first two qubits and again in the last two qubits. \n",
    "\n",
    "To save simulating a lengthy state preparation routine we use a hack: Compute $x \\otimes x$ manually and set the amplitude vector to this value. *Note: Since we normalized $x$ earlier, $x \\otimes x$ is also normalized!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x):\n",
    "\n",
    "    qm.QubitStateVector(onp.kron(x, x), wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"iris_scaled.txt\")\n",
    "X = data[:, :-1]\n",
    "normalization = np.sqrt(np.sum(X ** 2, -1))\n",
    "X = (X.T / normalization).T  # normalize each feature vector\n",
    "Y = data[:, -1]\n",
    "Y = Y*2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X= {}, Y = {}\".format(X[i], Y[i]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, the data is split into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = len(X)\n",
    "num_train = int(0.75*num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "X_train = X[index[: num_train]]\n",
    "Y_train = Y[index[: num_train]]\n",
    "X_val = X[index[num_train: ]]\n",
    "Y_val = Y[index[num_train: ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we optimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = GradientDescentOptimizer(0.01)\n",
    "batch_size = 3\n",
    "\n",
    "weights = np.array(weights0)\n",
    "for iteration in range(1):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size, ))\n",
    "    X_train_batch = X_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights = o.step(lambda w: cost(w, X_train_batch, Y_train_batch), weights)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, x=x)) for x in X_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, x=x)) for x in X_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    print(\"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "          \"\".format(iteration+1, cost(weights, X, Y), acc_train, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the continuous output of the variational classifier for the first two dimensions of the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "cm = plt.cm.RdBu\n",
    "\n",
    "# make data for decision regions\n",
    "xx, yy = np.meshgrid(np.linspace(-1.1, 1.1, 20), np.linspace(-1.1, 1.1, 20))\n",
    "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
    "predictions_grid = [variational_classifier(weights, x=x) for x in X_grid]\n",
    "Z = np.reshape(predictions_grid, xx.shape)\n",
    "\n",
    "# plot decision regions\n",
    "cnt = plt.contourf(xx, yy, Z, levels=np.arange(-1, 1.1, 0.1), cmap=cm, alpha=.8, extend='both')\n",
    "plt.colorbar(cnt, ticks=[-1, 0, 1])\n",
    "\n",
    "# plot data\n",
    "trf0 = [d for i, d in enumerate(X_train) if Y_train[i] == -1]\n",
    "trf1 = [d for i, d in enumerate(X_train) if Y_train[i] == 1]\n",
    "plt.scatter([c[0] for c in trf1], [c[1] for c in trf1], c='r', marker='^', edgecolors='k')\n",
    "plt.scatter([c[0] for c in trf0], [c[1] for c in trf0], c='r', marker='o', edgecolors='k')\n",
    "tes0 = [d for i, d in enumerate(X_val) if Y_val[i] == -1]\n",
    "tes1 = [d for i, d in enumerate(X_val) if Y_val[i] == 1]\n",
    "plt.scatter([c[0] for c in tes1], [c[1] for c in tes1], c='g', marker='^', edgecolors='k')\n",
    "plt.scatter([c[0] for c in tes0], [c[1] for c in tes0], c='g', marker='o', edgecolors='k')\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regularizer(weights):\n",
    "    \"\"\"L2 Regularizer penalty on weights\n",
    "\n",
    "    Args:\n",
    "        weights (array[float]): The array of trainable weights\n",
    "    Returns:\n",
    "        float: regularization penalty\n",
    "    \"\"\"\n",
    "    w_flat = weights.flatten()\n",
    "\n",
    "    # Compute the l2 norm\n",
    "    reg = onp.abs(onp.inner(w_flat, w_flat))\n",
    "\n",
    "    return reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
