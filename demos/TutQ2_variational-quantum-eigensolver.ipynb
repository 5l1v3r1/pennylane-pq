{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Q2 - Variational quantum eigensolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates a simplified variational quantum eigensolver (Peruzzo et al 2014). To showcase the hybrid principle of openqml, we first train a quantum circuit to minimize the energy expectation for a Hamiltonian $H$, \n",
    "\n",
    "$$ \\langle \\psi | H | \\psi \\rangle  = 0.1 \\langle \\psi_{v} | X_2 | \\psi_v \\rangle + 0.5 \\langle \\psi_v | Y_2 | \\psi_v \\rangle.  $$\n",
    "\n",
    "Here, $| \\psi_v \\rangle $ is the state after applying the quantum circuit which depends on trainable variables $v = \\{v_1, v_2\\}$, and $X_2$, $Y_2$ denote the Pauli-X and Pauli-Y operator acting on the second qubit. \n",
    "\n",
    "We then turn things around and use a fixed quantum circuit to prepare $| \\psi \\rangle $, but train the coefficients of the Hamiltonian to minimize\n",
    "\n",
    "$$ \\langle \\psi | H | \\psi \\rangle  = v_1 \\langle \\psi | X_2 | \\psi \\rangle + v_2 \\langle \\psi | Y_2 | \\psi \\rangle . $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimize the quantum circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside the openqml framework and gradient descent optimizer, we import the original numpy library. \n",
    "\n",
    "*Note: If we perform numpy operations in the cost function or any function the cost depends on, we have to use the openqml version of numpy imported as `from openqml import numpy as onp`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openqml as qm\n",
    "from openqml.optimize import GradientDescentOptimizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the projectq simulator as a device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qm.device('projectq.simulator', wires=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantum circuit of the variational eigensolver is an ansatz that defines a manifold of possible quantum states. We use a Hadamard, two rotations and a CNOT gate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansatz(vars):\n",
    "\n",
    "    qm.Hadamard([0])\n",
    "    qm.RX(vars[0], [0])\n",
    "    qm.RY(vars[1], [1])\n",
    "    qm.CNOT([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational eigensolvers requires us to evaluate expectations of different Pauli operators. In this example, the Hamiltonian is expressed by only two single-qubit Pauli operators, namely the X and Y operator applied to the first qubit. \n",
    "\n",
    "Since these operators do not commute, we need two quantum nodes, but they can reuse the same device we created. \n",
    "\n",
    "*NOTE: If the Pauli observables referred to different qubits, we could use one quantum function and return a tuple of expectations in only one quantum node: `return qm.expectation.PauliX(0), qm.expectation.PauliX(1)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qm.qnode(dev)\n",
    "def circuit_X(vars):\n",
    "    ansatz(vars)\n",
    "    return qm.expval.PauliX(1)\n",
    "\n",
    "\n",
    "@qm.qnode(dev)\n",
    "def circuit_Y(vars):\n",
    "    ansatz(vars)\n",
    "    return qm.expval.PauliY(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of a VQE, usually called a \"cost\", is simply a linear combination of the expectations, which defines the expectation of the Hamiltonian we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(vars):\n",
    "\n",
    "    expX = circuit_X(vars)\n",
    "    expY = circuit_Y(vars)\n",
    "\n",
    "    return 0.1*expX + 0.5*expY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cost defines the following landscape:\n",
    "\n",
    "*Note: To run the following cell you need the matplotlib library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig = plt.figure(figsize = (6, 4))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "X = np.linspace(-3.1, 3.1, 30)\n",
    "Y = np.linspace(-3.1, 3.1, 30)\n",
    "xx, yy = np.meshgrid(X, Y)\n",
    "Z = np.array([[cost([x, y]) for x in X] for y in Y]).reshape(len(Y), len(X))\n",
    "surf = ax.plot_surface(xx, yy, Z, cmap=cm.coolwarm, antialiased=False)\n",
    "\n",
    "ax.set_xlabel(\"v1\")\n",
    "ax.set_ylabel(\"v2\")\n",
    "ax.zaxis.set_major_locator(MaxNLocator(nbins = 5, prune = 'lower'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the gradient descent optimizer from Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     1: -0.004997917 | Variables: [ 0.0,-0.05]\n",
      "Cost after step     2: -0.009977124 | Variables: [ 0.0,-0.099938]\n",
      "Cost after step     3: -0.01491297 | Variables: [-3.4694e-19,-0.14969]\n",
      "Cost after step     4: -0.01978155 | Variables: [-3.4694e-19,-0.19913]\n",
      "Cost after step     5: -0.02456022 | Variables: [-3.4694e-19,-0.24814]\n",
      "Cost after step     6: -0.02922794 | Variables: [-3.4694e-19,-0.29661]\n",
      "Cost after step     7: -0.03376565 | Variables: [-3.4694e-19,-0.34443]\n",
      "Cost after step     8: -0.03815657 | Variables: [-1.7347e-18,-0.39149]\n",
      "Cost after step     9: -0.04238634 | Variables: [-1.7347e-18,-0.43771]\n",
      "Cost after step    10: -0.04644318 | Variables: [-3.1225e-18,-0.48299]\n",
      "Cost after step    11: -0.05031789 | Variables: [-3.1225e-18,-0.52727]\n",
      "Cost after step    12: -0.05400382 | Variables: [-3.1225e-18,-0.57048]\n",
      "Cost after step    13: -0.05749676 | Variables: [-3.1225e-18,-0.61256]\n",
      "Cost after step    14: -0.06079478 | Variables: [-3.1225e-18,-0.65347]\n",
      "Cost after step    15: -0.06389805 | Variables: [-3.1225e-18,-0.69317]\n",
      "Cost after step    16: -0.06680858 | Variables: [-3.1225e-18,-0.73163]\n",
      "Cost after step    17: -0.06953002 | Variables: [-3.1225e-18,-0.76884]\n",
      "Cost after step    18: -0.07206736 | Variables: [-3.1225e-18,-0.80477]\n",
      "Cost after step    19: -0.07442674 | Variables: [-3.1225e-18,-0.83944]\n",
      "Cost after step    20: -0.07661516 | Variables: [-3.1225e-18,-0.87283]\n"
     ]
    }
   ],
   "source": [
    "o = GradientDescentOptimizer(0.5)\n",
    "\n",
    "vars = np.array([0., 0.])\n",
    "vars_gd = [vars]\n",
    "for it in range(20):\n",
    "    vars = o.step(cost, vars)\n",
    "    vars_gd.append(vars)\n",
    "\n",
    "    print('Cost after step {:5d}: {: .7} | Variables: [{: .5},{: .5}]'\n",
    "          .format(it+1, cost(vars), vars[0], vars[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the path gradient descent took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6, 4))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "X = np.linspace(-3, 3, 20)\n",
    "Y = np.linspace(-3, 3, 20)\n",
    "xx, yy = np.meshgrid(X, Y)\n",
    "Z = np.array([[cost([x, y]) for x in X] for y in Y]).reshape(len(Y), len(X))\n",
    "surf = ax.plot_surface(xx, yy, Z, cmap=cm.coolwarm, antialiased=False)\n",
    "\n",
    "path_z = [cost(vars)+1e-8 for vars in vars_gd]\n",
    "path_x = [v[0] for v in vars_gd]\n",
    "path_y = [v[1] for v in vars_gd]\n",
    "ax.plot(path_x, path_y, path_z, c='green', marker='.', label=\"graddesc\")\n",
    "\n",
    "ax.set_xlabel(\"v1\")\n",
    "ax.set_ylabel(\"v2\")\n",
    "ax.zaxis.set_major_locator(MaxNLocator(nbins = 5, prune = 'lower'))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimizing the Hamiltonian coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of optimizing the circuit parameters, we can also use a fixed circuit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansatz():\n",
    "\n",
    "    qm.Hadamard([0])\n",
    "    qm.RX(-0.5, [0])\n",
    "    qm.RY( 0.5, [1])\n",
    "    qm.CNOT([0, 1])\n",
    "    \n",
    "    \n",
    "@qm.qnode(dev)\n",
    "def circuit_X():\n",
    "    \"\"\"Circuit measuring the X operator for the second qubit\"\"\"\n",
    "    ansatz()\n",
    "    return qm.expval.PauliX(1)\n",
    "\n",
    "\n",
    "@qm.qnode(dev)\n",
    "def circuit_Y():\n",
    "    \"\"\"Circuit measuring the Y operator for the second qubit\"\"\"\n",
    "    ansatz()\n",
    "    return qm.expval.PauliY(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and make the classical coefficients trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(vars):\n",
    "\n",
    "    expX = circuit_X()\n",
    "    expY = circuit_Y()\n",
    "\n",
    "    return vars[0]*expX + vars[1]*expY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this cost, ever smaller coefficients decrease the energy expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     1: -0.1149244 | Variables: [-0.23971, 0.0]\n",
      "Cost after step     2: -0.2298488 | Variables: [-0.47943, 0.0]\n",
      "Cost after step     3: -0.3447733 | Variables: [-0.71914, 0.0]\n",
      "Cost after step     4: -0.4596977 | Variables: [-0.95885, 0.0]\n",
      "Cost after step     5: -0.5746221 | Variables: [-1.1986, 0.0]\n",
      "Cost after step     6: -0.6895465 | Variables: [-1.4383, 0.0]\n",
      "Cost after step     7: -0.804471 | Variables: [-1.678, 0.0]\n",
      "Cost after step     8: -0.9193954 | Variables: [-1.9177, 0.0]\n",
      "Cost after step     9: -1.03432 | Variables: [-2.1574, 0.0]\n",
      "Cost after step    10: -1.149244 | Variables: [-2.3971, 0.0]\n",
      "Cost after step    11: -1.264169 | Variables: [-2.6368, 0.0]\n",
      "Cost after step    12: -1.379093 | Variables: [-2.8766, 0.0]\n",
      "Cost after step    13: -1.494018 | Variables: [-3.1163, 0.0]\n",
      "Cost after step    14: -1.608942 | Variables: [-3.356, 0.0]\n",
      "Cost after step    15: -1.723866 | Variables: [-3.5957, 0.0]\n",
      "Cost after step    16: -1.838791 | Variables: [-3.8354, 0.0]\n",
      "Cost after step    17: -1.953715 | Variables: [-4.0751, 0.0]\n",
      "Cost after step    18: -2.06864 | Variables: [-4.3148, 0.0]\n",
      "Cost after step    19: -2.183564 | Variables: [-4.5545, 0.0]\n",
      "Cost after step    20: -2.298488 | Variables: [-4.7943, 0.0]\n"
     ]
    }
   ],
   "source": [
    "o = GradientDescentOptimizer(0.5)\n",
    "\n",
    "vars = np.array([0., 0.])\n",
    "vars_gd = [vars]\n",
    "for it in range(20):\n",
    "    vars = o.step(cost, vars)\n",
    "    vars_gd.append(vars)\n",
    "\n",
    "\n",
    "    print('Cost after step {:5d}: {: .7} | Variables: [{: .5},{: .5}]'\n",
    "          .format(it+1, cost(vars), vars[0], vars[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the optimization landscape is nearly linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6, 4))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "X = np.linspace(-3, 3, 50)\n",
    "Y = np.linspace(-3, 3, 50)\n",
    "xx, yy = np.meshgrid(X, Y)\n",
    "Z = np.array([[cost([x, y]) for x in X] for y in Y]).reshape(len(Y), len(X))\n",
    "surf = ax.plot_surface(xx, yy, Z, cmap=cm.coolwarm, antialiased=False)\n",
    "\n",
    "path_z = [cost(vars)+1e-8 for vars in vars_gd]\n",
    "path_x = [v[0] for v in vars_gd]\n",
    "path_y = [v[1] for v in vars_gd]\n",
    "ax.plot(path_x, path_y, path_z, c='green', marker='.', label=\"graddesc\")\n",
    "\n",
    "ax.set_xlabel(\"v1\")\n",
    "ax.set_ylabel(\"v2\")\n",
    "ax.zaxis.set_major_locator(MaxNLocator(nbins = 5, prune = 'lower'))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"figures/vqe_c_landscape_gd.png\" width=\"450\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizing classical and quantum parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also optimize \"classical\" and \"quantum\" weights together by combining the two approaches from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansatz(vars):\n",
    "\n",
    "    qm.Hadamard([0])\n",
    "    qm.RX(vars[0], [0])\n",
    "    qm.RY(vars[1], [1])\n",
    "    qm.CNOT([0, 1])\n",
    "\n",
    "\n",
    "@qm.qnode(dev)\n",
    "def circuit_X(vars):\n",
    "    ansatz(vars)\n",
    "    return qm.expval.PauliX(1)\n",
    "\n",
    "\n",
    "@qm.qnode(dev)\n",
    "def circuit_Y(vars):\n",
    "    ansatz(vars)\n",
    "    return qm.expval.PauliY(1)\n",
    "\n",
    "\n",
    "def cost(vars):\n",
    "\n",
    "    expX = circuit_X(vars)\n",
    "    expY = circuit_Y(vars)\n",
    "\n",
    "    return vars[2]*expX + vars[3]*expY\n",
    "\n",
    "weights0 = np.array([0., 0., 0., 0.])\n",
    "print('Initial weights:', weights0)\n",
    "\n",
    "o = GradientDescentOptimizer(0.5)\n",
    "weights = weights0\n",
    "for iteration in np.arange(1, 21):\n",
    "    weights = o.step(cost, weights)\n",
    "    print('Cost after step {:5d}: {: 0.7f}'.format(iteration, cost(weights)))\n",
    "print('Optimized weights:', weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
